{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f15b95",
   "metadata": {},
   "source": [
    "### <center>**Classification Supervisée et Evaluation des Modèles**</center>\n",
    "\n",
    "#### **1. Définition du Variable Cible et des Features :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "352d3c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.825781</td>\n",
       "      <td>0.863078</td>\n",
       "      <td>0.055277</td>\n",
       "      <td>0.711208</td>\n",
       "      <td>0.839238</td>\n",
       "      <td>0.266103</td>\n",
       "      <td>0.612059</td>\n",
       "      <td>1.437767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.802604</td>\n",
       "      <td>-1.206933</td>\n",
       "      <td>-0.439687</td>\n",
       "      <td>0.169719</td>\n",
       "      <td>-0.889534</td>\n",
       "      <td>-0.839820</td>\n",
       "      <td>-0.324994</td>\n",
       "      <td>-0.050575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.152449</td>\n",
       "      <td>2.013084</td>\n",
       "      <td>-0.614580</td>\n",
       "      <td>0.209057</td>\n",
       "      <td>1.840419</td>\n",
       "      <td>-1.462846</td>\n",
       "      <td>0.749586</td>\n",
       "      <td>0.047687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.802604</td>\n",
       "      <td>-1.075504</td>\n",
       "      <td>-0.439687</td>\n",
       "      <td>-0.493010</td>\n",
       "      <td>-0.475488</td>\n",
       "      <td>-0.580889</td>\n",
       "      <td>-1.063014</td>\n",
       "      <td>-1.247065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.703581</td>\n",
       "      <td>0.501647</td>\n",
       "      <td>-3.273965</td>\n",
       "      <td>0.711208</td>\n",
       "      <td>0.479229</td>\n",
       "      <td>1.453088</td>\n",
       "      <td>4.158488</td>\n",
       "      <td>0.143015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       "0     0.825781  0.863078       0.055277       0.711208  0.839238  0.266103   \n",
       "1    -0.802604 -1.206933      -0.439687       0.169719 -0.889534 -0.839820   \n",
       "2     1.152449  2.013084      -0.614580       0.209057  1.840419 -1.462846   \n",
       "3    -0.802604 -1.075504      -0.439687      -0.493010 -0.475488 -0.580889   \n",
       "4    -1.703581  0.501647      -3.273965       0.711208  0.479229  1.453088   \n",
       "\n",
       "   DiabetesPedigreeFunction       Age  \n",
       "0                  0.612059  1.437767  \n",
       "1                 -0.324994 -0.050575  \n",
       "2                  0.749586  0.047687  \n",
       "3                 -1.063014 -1.247065  \n",
       "4                  4.158488  0.143015  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../data/cleaned/data.csv\")\n",
    "\n",
    "y = data['Cluster']\n",
    "X = data.drop(columns=['Cluster', 'risk_category'])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5802c37",
   "metadata": {},
   "source": [
    "#### **2. Division des Données :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da77418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51b5b7e",
   "metadata": {},
   "source": [
    "Le **``stratify``** garantit que la proportion des clusters (0/1) reste la même dans train et test. C’est essentiel quand on a déséquilibre potentiel.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **4. Gestion du Déséquilibre :**\n",
    "\n",
    "##### **4.1. C’est quoi un déséquilibre des classes ?**\n",
    "\n",
    "Quand on fait de la classification, on a souvent un jeu de données avec une variable cible (``y``) qui contient plusieurs classes.\n",
    "\n",
    "Dans notre Dataset :\n",
    "\n",
    "- ``0`` = diabète\n",
    "\n",
    "- ``1`` = pas de diabète\n",
    "\n",
    "Si dans notre dataset on a :\n",
    "\n",
    "- ``90`` % de classe ``0``.\n",
    "\n",
    "- ``10`` % de classe ``1``.\n",
    "\n",
    "Alors notre Dataset est **_déséquilibré_**.\n",
    "\n",
    "Le modèle risque d’apprendre à prédire presque toujours la classe majoritaire (0), car elle domine les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7253c0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant sur-échantillonnage : Counter({0: 320, 1: 294})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Avant sur-échantillonnage :\", Counter(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc995a6",
   "metadata": {},
   "source": [
    "##### **4.2. RandomOverSampler :**\n",
    "\n",
    "- ``RandomOverSampler`` est une technique de rééchantillonnage (resampling) fournie par la bibliothèque imblearn (``imbalanced-learn``).\n",
    "\n",
    "- Elle sert à équilibrer les classes en dupliquant aléatoirement des exemples de la classe minoritaire.\n",
    "\n",
    "- Il repère la classe minoritaire (celle qui a le moins d’exemples).\n",
    "\n",
    "- Il duplique aléatoirement certains échantillons de cette classe jusqu’à ce que les deux classes aient le même nombre d’exemples.\n",
    "\n",
    "C’est une méthode simple mais efficace pour donner au modèle plus de données de la classe rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "388c1519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après sur-échantillonnage : Counter({0: 320, 1: 320})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Après sur-échantillonnage :\", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5edf22f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **5. Entraînement des Modèles :**\n",
    "\n",
    "##### **5.1. RandomForest :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09bea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor : \n",
      "- Accuracy : 0.9415584415584416\n",
      "- Precision : 0.941603466717622\n",
      "- ReCall : 0.9415584415584416\n",
      "- F1 Score : 0.9415411562705263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "prec_rf = precision_score(y_test, y_pred_rf, average='weighted', zero_division=0)\n",
    "rec_rf = recall_score(y_test, y_pred_rf, average='weighted', zero_division=0)\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"RandomForestRegressor : \")\n",
    "\n",
    "print(f\"- Accuracy : {acc_rf}\")\n",
    "print(f\"- Precision : {prec_rf}\")\n",
    "print(f\"- ReCall : {rec_rf}\")\n",
    "print(f\"- F1 Score : {f1_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fdfb92",
   "metadata": {},
   "source": [
    "##### **5.2. SVM :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc5a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC : \n",
      "- Accuracy : 0.974025974025974\n",
      "- Precision : 0.9743207334670749\n",
      "- ReCall : 0.974025974025974\n",
      "- F1 Score : 0.9740084032321477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(probability=True, random_state=42)\n",
    "\n",
    "svm.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "prec_svm = precision_score(y_test, y_pred_svm, average='weighted', zero_division=0)\n",
    "rec_svm = recall_score(y_test, y_pred_svm, average='weighted', zero_division=0)\n",
    "f1_svm = f1_score(y_test, y_pred_svm, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"SVC : \")\n",
    "\n",
    "print(f\"- Accuracy : {acc_svm}\")\n",
    "print(f\"- Precision : {prec_svm}\")\n",
    "print(f\"- ReCall : {rec_svm}\")\n",
    "print(f\"- F1 Score : {f1_svm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1565fd1",
   "metadata": {},
   "source": [
    "##### **5.3. Gradient Boosting :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62d2752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier : \n",
      "- Accuracy : 0.948051948051948\n",
      "- Precision : 0.9483027135466161\n",
      "- ReCall : 0.948051948051948\n",
      "- F1 Score : 0.948016806464295\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import  GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "gbc.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred_gbc = gbc.predict(X_test)\n",
    "\n",
    "acc_gbc = accuracy_score(y_test, y_pred_gbc)\n",
    "prec_gbc = precision_score(y_test, y_pred_gbc, average='weighted', zero_division=0)\n",
    "rec_gbc = recall_score(y_test, y_pred_gbc, average='weighted', zero_division=0)\n",
    "f1_gbc = f1_score(y_test, y_pred_gbc, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"GradientBoostingClassifier : \")\n",
    "\n",
    "print(f\"- Accuracy : {acc_gbc}\")\n",
    "print(f\"- Precision : {prec_gbc}\")\n",
    "print(f\"- ReCall : {rec_gbc}\")\n",
    "print(f\"- F1 Score : {f1_gbc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb13ae",
   "metadata": {},
   "source": [
    "##### **5.4. Decision Tree :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b7f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier : \n",
      "- Accuracy : 0.8376623376623377\n",
      "- Precision : 0.8430473310839848\n",
      "- ReCall : 0.8376623376623377\n",
      "- F1 Score : 0.8374774506296843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "dt.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "prec_dt = precision_score(y_test, y_pred_dt, average='weighted', zero_division=0)\n",
    "rec_dt = recall_score(y_test, y_pred_dt, average='weighted', zero_division=0)\n",
    "f1_dt = f1_score(y_test, y_pred_dt, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"DecisionTreeClassifier : \")\n",
    "\n",
    "print(f\"- Accuracy : {acc_dt}\")\n",
    "print(f\"- Precision : {prec_dt}\")\n",
    "print(f\"- ReCall : {rec_dt}\")\n",
    "print(f\"- F1 Score : {f1_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a6670",
   "metadata": {},
   "source": [
    "##### **5.5. Régression Logistique :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46294c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression : \n",
      "- Accuracy : 0.987012987012987\n",
      "- Precision : 0.987012987012987\n",
      "- ReCall : 0.987012987012987\n",
      "- F1 Score : 0.987012987012987\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "lr.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "prec_lr = precision_score(y_test, y_pred_lr, average='weighted', zero_division=0)\n",
    "rec_lr = recall_score(y_test, y_pred_lr, average='weighted', zero_division=0)\n",
    "f1_lr = f1_score(y_test, y_pred_lr, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"LogisticRegression : \")\n",
    "\n",
    "print(f\"- Accuracy : {acc_lr}\")\n",
    "print(f\"- Precision : {prec_lr}\")\n",
    "print(f\"- ReCall : {rec_lr}\")\n",
    "print(f\"- F1 Score : {f1_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734b923",
   "metadata": {},
   "source": [
    "##### **5.6. XGB :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee13008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier : \n",
      "- Accuracy : 0.948051948051948\n",
      "- Precision : 0.9483027135466161\n",
      "- ReCall : 0.948051948051948\n",
      "- F1 Score : 0.948016806464295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [16:48:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "xgb.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "prec_xgb = precision_score(y_test, y_pred_xgb, average='weighted', zero_division=0)\n",
    "rec_xgb = recall_score(y_test, y_pred_xgb, average='weighted', zero_division=0)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"XGBClassifier : \")\n",
    "\n",
    "print(f\"- Accuracy : {acc_xgb}\")\n",
    "print(f\"- Precision : {prec_xgb}\")\n",
    "print(f\"- ReCall : {rec_xgb}\")\n",
    "print(f\"- F1 Score : {f1_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809cac1",
   "metadata": {},
   "source": [
    "##### **5.7. Enregistrement des Performances :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28450b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'RandomForestClassifier': [0.9415584415584416, 0.941603466717622, 0.9415584415584416, 0.9415411562705263]}, {'SVM': [0.974025974025974, 0.9743207334670749, 0.974025974025974, 0.9740084032321477]}, {'GradientBoostingClassifier': [0.948051948051948, 0.9483027135466161, 0.948051948051948, 0.948016806464295]}, {'DecisionTreeClassifier': [0.8376623376623377, 0.8430473310839848, 0.8376623376623377, 0.8374774506296843]}, {'LogisticRegression': [0.987012987012987, 0.987012987012987, 0.987012987012987, 0.987012987012987]}, {'XGBClassifier': [0.948051948051948, 0.9483027135466161, 0.948051948051948, 0.948016806464295]}]\n"
     ]
    }
   ],
   "source": [
    "models = ['RandomForestClassifier', 'SVM', 'GradientBoostingClassifier', 'DecisionTreeClassifier', 'LogisticRegression', 'XGBClassifier']\n",
    "\n",
    "accuracies = [acc_rf, acc_svm, acc_gbc, acc_dt, acc_lr, acc_xgb]\n",
    "\n",
    "precisions = [prec_rf, prec_svm, prec_gbc, prec_dt, prec_lr, prec_xgb]\n",
    "\n",
    "recalls = [rec_rf, rec_svm, rec_gbc, rec_dt, rec_lr, rec_xgb]\n",
    "\n",
    "f1_s = [f1_rf, f1_svm, f1_gbc, f1_dt, f1_lr, f1_xgb]\n",
    "\n",
    "performances = []\n",
    "\n",
    "for i, model in enumerate(models) :\n",
    "    performances.append({model : [accuracies[i], precisions[i], recalls[i], f1_s[i]]})\n",
    "\n",
    "print(performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85995923",
   "metadata": {},
   "source": [
    "#### **6. GridSearchCV :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eeb27cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : RandomForestClassifier\n",
      "Best parameters : {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best cross-validation score : 0.9469\n",
      "\n",
      "Model : GradientBoostingClassifier\n",
      "Best parameters : {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best cross-validation score : 0.9437\n",
      "\n",
      "Model : SVC\n",
      "Best parameters : {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best cross-validation score : 0.9922\n",
      "\n",
      "Model : DecisionTreeClassifier\n",
      "Best parameters : {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best cross-validation score : 0.8766\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : LogisticRegression\n",
      "Best parameters : {'C': 10, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best cross-validation score : 0.9906\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "60 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [0.9734375       nan 0.96875   0.9875    0.9890625 0.9890625       nan\n",
      "       nan       nan 0.9890625       nan 0.9875    0.9875    0.9875\n",
      " 0.9875          nan       nan       nan 0.9890625       nan 0.990625\n",
      " 0.990625  0.990625  0.990625        nan       nan       nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "models = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    'SVC': SVC(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "    'LogisticRegression': LogisticRegression()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto', 0.1]\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'max_depth': [None, 5, 10],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'penalty': ['l1', 'l2', 'none'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['liblinear', 'lbfgs', 'saga']\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[name], cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "    print(f\"Model : {name}\")\n",
    "    print(f\"Best parameters : {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score : {grid_search.best_score_:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
